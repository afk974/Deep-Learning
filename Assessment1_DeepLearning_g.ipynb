{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afk974/Deep-Learning/blob/main/Assessment1_DeepLearning_g.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpRNh1-L8zuk"
      },
      "source": [
        "## Assessment 1: Deep Learning\n",
        "\n",
        "1) Answer all questions.\n",
        "2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc.\n",
        "3) Copy this notebook to your google drive (click **FILE** > **save a copy in Drive**)\n",
        "4) Upload the answer notebook to your github. \n",
        "5) Submit the assessment by sharing the link to your answer notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRauIpz8zun"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION 1** \n",
        "\n",
        "One day while wandering around a clothing store at KL East Mall, you stumbled upon a person who is choosing a dress for Hari Raya. It turns out that the person is visually impaired and had a hard time distinguishing between an abaya and a kebaya. To help people with the similar situation, you then decided to develop an AI system to identify the type of clothes using a Convolutional Neural Networks (ConvNet). In order to train the network, you decide to use the Fashion MNIST dataset which is freely available on Pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzzvkxpn8zuo"
      },
      "source": [
        "a) Given the problem, what is the most appropriate loss function to use? Justify your answer. **[5 marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hERYSq8zuo"
      },
      "source": [
        "\n",
        "<span style=\"color:blue\">\n",
        "    ANSWER: Cross entropy loss function because it can calculate the performance for classification application using the output between 0 and 1.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6A4Pmj8zuo"
      },
      "source": [
        "b) Create and train a ConvNet corresponding to the following CNN architecture (with a modification of the final layer to address the number of classes). Please include **[10 marks]**:\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) Define your training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/LeNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Ue0OHCL8zup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcbf0fa-55cf-42ac-bac7-59faf4948b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "# 1) The dataloader to load the train and test datasets.\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize(32),\n",
        "     transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('T-shirt/Top', 'Trouser', 'Pullover', 'Dress',\n",
        "       'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) The model definition\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        #x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "pYvrF7SFMeJM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-ksjzd2Wes2",
        "outputId": "cb8e5a7e-0797-412b-d43a-440c297d22eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Define your training loop.\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'fashion_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n"
      ],
      "metadata": {
        "id": "c9FS53vcMjBo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 10 epochs\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "t-nfvu0-NQeW",
        "outputId": "0b8128b5-fda8-4391-a2d4-5a0aa013d48f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.8497, Accuracy: 70.0900%, \n",
            "\t\tValidation : Loss : 0.4778, Accuracy: 82.2700%, Time: 48.1426s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4156, Accuracy: 84.8900%, \n",
            "\t\tValidation : Loss : 0.3912, Accuracy: 85.7100%, Time: 39.8241s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3548, Accuracy: 86.8950%, \n",
            "\t\tValidation : Loss : 0.3548, Accuracy: 87.1300%, Time: 42.0541s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3223, Accuracy: 88.0467%, \n",
            "\t\tValidation : Loss : 0.3662, Accuracy: 86.5700%, Time: 40.1079s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2993, Accuracy: 88.9933%, \n",
            "\t\tValidation : Loss : 0.3332, Accuracy: 88.0800%, Time: 40.2180s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2824, Accuracy: 89.6317%, \n",
            "\t\tValidation : Loss : 0.3247, Accuracy: 88.3500%, Time: 40.0640s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.2681, Accuracy: 90.0083%, \n",
            "\t\tValidation : Loss : 0.3188, Accuracy: 88.6200%, Time: 40.5276s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2556, Accuracy: 90.4783%, \n",
            "\t\tValidation : Loss : 0.3092, Accuracy: 88.8400%, Time: 40.0354s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2459, Accuracy: 90.7783%, \n",
            "\t\tValidation : Loss : 0.3017, Accuracy: 89.1400%, Time: 39.9007s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2351, Accuracy: 91.2383%, \n",
            "\t\tValidation : Loss : 0.3012, Accuracy: 89.4700%, Time: 39.9064s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "# Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BeOvL579MlRz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "83e23f3c-e7f6-49e8-c054-41157ea90e47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qzl8sRuvCOEA",
        "outputId": "6bb47d11-9ac1-4bcb-d1e0-3c5972fc232b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Zjsjxq8zuq"
      },
      "source": [
        "c) Replace your defined CNN in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the Fashion MNIST dataset. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D4joDd5u8zur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "dd15c865fd0b44389aaaab9dd01400b6",
            "fe04b5a5e99547dcaf0efcfc42783bb5",
            "84b12ece4ac2425f8a8df7559471d260",
            "0fce803cfb3048308f0e60830a8b1a55",
            "bf1a697c93854a76bc6b33ab17c109ed",
            "aca63cd1c1de4edca996d116f3bbf63c",
            "a7adca183b6a4d95a283a61070831910",
            "ad8a540818f1463198a35f1f3a3a614d",
            "e8b13bdcbdd149c7a2fe43fd5e884dc9",
            "b6234a07ab18488e96d61ee410724c5e",
            "b8e9a21398d34509919ef007397a7d8e"
          ]
        },
        "outputId": "c4fa0bb0-c558-4e92-8cb4-8bfc862de54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/109M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd15c865fd0b44389aaaab9dd01400b6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_ft = models.convnext_tiny(pretrained=True)\n",
        "#num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer_ft, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TqvlLMpbf-9",
        "outputId": "2557cfbb-0e51-475e-a52e-bb5f500e8831"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.7196, Accuracy: 73.7483%, \n",
            "\t\tValidation : Loss : 0.4046, Accuracy: 85.2900%, Time: 209.3894s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3261, Accuracy: 88.0067%, \n",
            "\t\tValidation : Loss : 0.3129, Accuracy: 88.6200%, Time: 197.6347s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.2648, Accuracy: 90.2550%, \n",
            "\t\tValidation : Loss : 0.2754, Accuracy: 89.8600%, Time: 196.1009s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2241, Accuracy: 91.6750%, \n",
            "\t\tValidation : Loss : 0.2850, Accuracy: 89.2000%, Time: 195.2243s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.1957, Accuracy: 92.7933%, \n",
            "\t\tValidation : Loss : 0.2465, Accuracy: 91.1800%, Time: 194.2015s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.1711, Accuracy: 93.6050%, \n",
            "\t\tValidation : Loss : 0.2633, Accuracy: 90.7300%, Time: 215.0794s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1495, Accuracy: 94.3867%, \n",
            "\t\tValidation : Loss : 0.2463, Accuracy: 91.5600%, Time: 195.4873s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1283, Accuracy: 95.1450%, \n",
            "\t\tValidation : Loss : 0.2517, Accuracy: 91.3600%, Time: 194.6280s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.1103, Accuracy: 95.8883%, \n",
            "\t\tValidation : Loss : 0.2626, Accuracy: 92.0400%, Time: 194.1124s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.0981, Accuracy: 96.2883%, \n",
            "\t\tValidation : Loss : 0.2936, Accuracy: 91.8300%, Time: 192.6865s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uCpzFC8zur"
      },
      "source": [
        "d) Using model-centric methods, propose two (2) strategies that can be used to increase the accuracy of the model on the testing dataset. **[5 marks]**\n",
        "\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Two model-centric techniques that I propose are: Changing the learning rate or use different pretrained model for the dataset.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FIMfUfz8zur"
      },
      "source": [
        "e) Next, implement the two proposed model-centric techniques for the same problem as in the previous question. **[15 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9UIGCk5K8zus",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "95db33b2867b4d9aa1762c09816ff54f",
            "fc59315ff3f043bb8d3adf96da7aafc1",
            "51e6ad45015a46b5ba1cdf0c56df2f5d",
            "f8c731b0d0e44de8b6b5806f357e0f55",
            "441dcea1e5dc44228058b148a1633460",
            "c4d97d9a3afd42509a725edf2e0166f6",
            "fba2092975c64efca2f8091b39523fe8",
            "2686506dafc14a7c8d09b8c22efc1f5f",
            "d650a14214a14c49a03c08b1fbe8d308",
            "7eb58b1a43bd462bac605146ff7e0e3f",
            "fdb581bfe5cd46848a01489bc8af0908"
          ]
        },
        "outputId": "04d76598-919f-42a6-dcda-47298da6977b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/49.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95db33b2867b4d9aa1762c09816ff54f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_ft = models.googlenet(pretrained=True)\n",
        "#num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer_ft, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owoCwPrYJg49",
        "outputId": "f34afb08-46a6-4f07-9ce6-711ec47139ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.1523, Accuracy: 94.4967%, \n",
            "\t\tValidation : Loss : 0.2255, Accuracy: 92.1100%, Time: 212.6750s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.1463, Accuracy: 94.6750%, \n",
            "\t\tValidation : Loss : 0.2299, Accuracy: 91.8000%, Time: 206.1342s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.1347, Accuracy: 95.2783%, \n",
            "\t\tValidation : Loss : 0.2639, Accuracy: 90.9100%, Time: 206.8019s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.1308, Accuracy: 95.3000%, \n",
            "\t\tValidation : Loss : 0.2379, Accuracy: 92.0300%, Time: 206.7181s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.1222, Accuracy: 95.5233%, \n",
            "\t\tValidation : Loss : 0.2445, Accuracy: 91.9700%, Time: 208.8536s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.1137, Accuracy: 95.9717%, \n",
            "\t\tValidation : Loss : 0.2325, Accuracy: 92.2900%, Time: 206.1297s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1033, Accuracy: 96.3050%, \n",
            "\t\tValidation : Loss : 0.2482, Accuracy: 92.0100%, Time: 207.4020s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1005, Accuracy: 96.4100%, \n",
            "\t\tValidation : Loss : 0.2425, Accuracy: 91.8300%, Time: 208.0485s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.0971, Accuracy: 96.4633%, \n",
            "\t\tValidation : Loss : 0.2565, Accuracy: 92.1200%, Time: 207.9083s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.0867, Accuracy: 96.9317%, \n",
            "\t\tValidation : Loss : 0.2485, Accuracy: 92.0300%, Time: 211.0729s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPPxsCX8zus"
      },
      "source": [
        "f) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Yes, there is a small improvement in the training and validation accuracy because the different model will have different weights for each layer which make the results will be different from each other while the learning rate can help to fine tune a bit to get a better result. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVArqW8h8zus"
      },
      "source": [
        "g) In real applications, data-centric strategies are essential to train robust deep learning models. Give two (2) examples of such strategies and discuss how the strategies helps improving the model accuracy. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Flip horizontally the image and rotate the image as the new image </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zifLt-s8zut"
      },
      "source": [
        "h) Next, implement the two proposed data-centric techniques for the same problem as in the previous question. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rHNqMSvg8zut"
      },
      "outputs": [],
      "source": [
        "transforms = {\n",
        "    transforms.Compose([\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5],  [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('T-shirt/Top', 'Trouser', 'Pullover', 'Dress',\n",
        "       'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbJgDjINgEAr",
        "outputId": "f5309caf-2c2a-4966-9a54-6ea07e333bcf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.googlenet(pretrained=True)\n",
        "#num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.005, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7jFqrYzgfVE",
        "outputId": "cc83b385-b5f2-4c2b-9dae-751d54a5ae09"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer_ft, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZBKpkFxgqx7",
        "outputId": "583d4dc9-6244-4c04-9b19-0b3b2ba1585d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.7214, Accuracy: 77.2100%, \n",
            "\t\tValidation : Loss : 0.4051, Accuracy: 85.2000%, Time: 208.9194s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3689, Accuracy: 87.3250%, \n",
            "\t\tValidation : Loss : 0.3025, Accuracy: 89.6600%, Time: 207.6041s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.2963, Accuracy: 89.5850%, \n",
            "\t\tValidation : Loss : 0.2714, Accuracy: 90.5400%, Time: 205.4275s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2628, Accuracy: 90.7000%, \n",
            "\t\tValidation : Loss : 0.2575, Accuracy: 90.9500%, Time: 204.2032s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2373, Accuracy: 91.5633%, \n",
            "\t\tValidation : Loss : 0.2486, Accuracy: 91.4100%, Time: 203.1594s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2176, Accuracy: 92.3717%, \n",
            "\t\tValidation : Loss : 0.3545, Accuracy: 87.2100%, Time: 203.3409s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1994, Accuracy: 92.9283%, \n",
            "\t\tValidation : Loss : 0.2299, Accuracy: 91.9200%, Time: 205.6661s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1871, Accuracy: 93.2900%, \n",
            "\t\tValidation : Loss : 0.2330, Accuracy: 91.6700%, Time: 205.6959s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.1820, Accuracy: 93.5833%, \n",
            "\t\tValidation : Loss : 0.2477, Accuracy: 91.2400%, Time: 205.3258s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.1626, Accuracy: 94.2417%, \n",
            "\t\tValidation : Loss : 0.2423, Accuracy: 91.3500%, Time: 205.4060s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCy3b5888zut"
      },
      "source": [
        "**QUESTION 2** **[35 marks]**\n",
        "\n",
        "Firstly, watch this video:\n",
        "\n",
        "https://drive.google.com/file/d/1bsypahR7I3f_R3DXkfw_tf0BrbCHxE_O/view?usp=sharing\n",
        "\n",
        "This video shows an example of masked face recognition where the deep learning model is able to detect and classify your face even when wearing a face mask. Using the end-to-end object detection pipeline that you have learned, develop your own masked face recognition such that the model should recognize your face even on face mask while recognize other persons as \"others\".\n",
        "\n",
        "Deliverables for this question are:\n",
        "\n",
        "- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n",
        "- a short video (~10 secs) containing your face and your friends faces (for inference)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ve5qqQ4qEF_",
        "outputId": "070f697b-0cc8-43a4-a9a7-f5b1556ca3f2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 13553, done.\u001b[K\n",
            "remote: Counting objects: 100% (355/355), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 13553 (delta 254), reused 308 (delta 212), pack-reused 13198\u001b[K\n",
            "Receiving objects: 100% (13553/13553), 13.27 MiB | 31.60 MiB/s, done.\n",
            "Resolving deltas: 100% (9313/9313), done.\n",
            "/content/yolov5/yolov5\n",
            "Setup complete. Using torch 1.12.1+cu113 (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB9FzQBl4Rer",
        "outputId": "02c0764f-0a81-49b1-b284-4525d0d558c6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=ultralytics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "9oIfLdzS8zut"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"7cYcdgNrmJLs7CGiH9fN\")\n",
        "project = rf.workspace(\"image-segmentation-u6vsb\").project(\"face-detection-webiz\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ],
      "metadata": {
        "id": "HozmEwS6d311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea13e1a-eabd-4858-fbc2-708b17121861"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in /content/datasets/Face-Detection-1 to yolov5pytorch: 100% [6195729 / 6195729] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to /content/datasets/Face-Detection-1 in yolov5pytorch:: 100%|██████████| 686/686 [00:00<00:00, 2018.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "id": "afX1j1Btd832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0ff947-b2ee-4eb6-b2b6-e4d0838f836e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/datasets/Face-Detection-1/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=150, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.2-187-g5ef69ef Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 132MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/Face-Detection-1/train/labels' images and labels...294 found, 0 missing, 0 empty, 0 corrupt: 100% 294/294 [00:00<00:00, 1982.83it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/Face-Detection-1/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 294/294 [00:00<00:00, 445.10it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/Face-Detection-1/valid/labels' images and labels...28 found, 0 missing, 0 empty, 0 corrupt: 100% 28/28 [00:00<00:00, 1285.79it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/Face-Detection-1/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 28/28 [00:00<00:00, 146.06it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.87 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 150 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      0/149      1.74G      0.112    0.02309    0.02969         20        416: 100% 19/19 [00:05<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.49s/it]\n",
            "                   all         28         49    0.00271      0.555    0.00559    0.00102\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      1/149      1.97G    0.08618    0.02942     0.0224         17        416: 100% 19/19 [00:02<00:00,  6.59it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.15it/s]\n",
            "                   all         28         49      0.153      0.204       0.13     0.0297\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      2/149      1.97G    0.07124    0.02723    0.01706         14        416: 100% 19/19 [00:02<00:00,  6.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.23it/s]\n",
            "                   all         28         49     0.0922      0.379     0.0977      0.027\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      3/149      1.97G    0.06546    0.02598    0.01479         16        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.20it/s]\n",
            "                   all         28         49      0.185      0.681      0.192     0.0679\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      4/149      1.97G    0.06621    0.02306    0.01293         24        416: 100% 19/19 [00:02<00:00,  6.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.27it/s]\n",
            "                   all         28         49      0.215      0.721       0.25     0.0691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      5/149      1.97G    0.06526    0.02076    0.01127         22        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.88it/s]\n",
            "                   all         28         49      0.516       0.52      0.566      0.174\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      6/149      1.97G    0.06183    0.02036   0.008492         21        416: 100% 19/19 [00:02<00:00,  6.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.57it/s]\n",
            "                   all         28         49      0.355      0.531      0.399     0.0819\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      7/149      1.97G    0.05592     0.0199   0.006133         21        416: 100% 19/19 [00:02<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.07it/s]\n",
            "                   all         28         49      0.797      0.581      0.663      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      8/149      1.97G    0.05204    0.01758   0.004859         16        416: 100% 19/19 [00:02<00:00,  6.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.52it/s]\n",
            "                   all         28         49      0.661      0.744      0.741      0.442\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      9/149      1.97G    0.04959    0.01765    0.00427         18        416: 100% 19/19 [00:02<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.39it/s]\n",
            "                   all         28         49      0.874      0.795      0.855      0.421\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     10/149      1.97G    0.04649     0.0154   0.003602         11        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.73it/s]\n",
            "                   all         28         49      0.613      0.933       0.88      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     11/149      1.97G    0.04471    0.01638   0.003724         25        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.72it/s]\n",
            "                   all         28         49      0.529      0.912      0.691      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     12/149      1.97G    0.04544    0.01548   0.002645         23        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.81it/s]\n",
            "                   all         28         49       0.77      0.789      0.842       0.34\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     13/149      1.97G    0.04475    0.01512   0.002644         18        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.08it/s]\n",
            "                   all         28         49       0.93      0.897      0.945      0.419\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     14/149      1.97G    0.04132    0.01518   0.002636         21        416: 100% 19/19 [00:02<00:00,  6.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.97it/s]\n",
            "                   all         28         49      0.828      0.949       0.88      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     15/149      1.97G    0.04035    0.01373   0.003031         12        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.84it/s]\n",
            "                   all         28         49      0.811      0.962      0.866      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     16/149      1.97G     0.0404    0.01446   0.003065         17        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.90it/s]\n",
            "                   all         28         49      0.878      0.962      0.952       0.54\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     17/149      1.97G     0.0402    0.01397   0.002135         11        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.33it/s]\n",
            "                   all         28         49      0.928      0.949      0.978      0.606\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     18/149      1.97G    0.03732    0.01306   0.002048         10        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.81it/s]\n",
            "                   all         28         49      0.918      0.946      0.953      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     19/149      1.97G    0.03737     0.0141   0.001969         17        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.04it/s]\n",
            "                   all         28         49      0.956      0.949      0.989      0.555\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     20/149      1.97G    0.03723    0.01284   0.002389          6        416: 100% 19/19 [00:02<00:00,  6.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.87it/s]\n",
            "                   all         28         49       0.97      0.949      0.992      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     21/149      1.97G    0.03602     0.0136   0.002587         22        416: 100% 19/19 [00:02<00:00,  6.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.66it/s]\n",
            "                   all         28         49      0.964      0.949      0.981      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     22/149      1.97G    0.03478     0.0133   0.001438         14        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.45it/s]\n",
            "                   all         28         49      0.965      0.994      0.994      0.455\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     23/149      1.97G    0.03489    0.01322   0.001536         12        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.34it/s]\n",
            "                   all         28         49      0.979      0.968      0.993        0.4\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     24/149      1.97G    0.03403    0.01243   0.001454         26        416: 100% 19/19 [00:02<00:00,  7.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.78it/s]\n",
            "                   all         28         49      0.943       0.96      0.979      0.375\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     25/149      1.97G    0.03433    0.01245   0.001704         25        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.34it/s]\n",
            "                   all         28         49      0.964      0.962      0.984       0.45\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     26/149      1.97G    0.03262    0.01184    0.00174         16        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.60it/s]\n",
            "                   all         28         49       0.98      0.962      0.987      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     27/149      1.97G    0.03462    0.01328    0.00154         28        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.58it/s]\n",
            "                   all         28         49      0.983      0.973      0.992      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     28/149      1.97G    0.03352    0.01263   0.001415         13        416: 100% 19/19 [00:02<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.80it/s]\n",
            "                   all         28         49      0.974      0.962      0.993      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     29/149      1.97G    0.03327    0.01204   0.001599         23        416: 100% 19/19 [00:02<00:00,  6.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.15it/s]\n",
            "                   all         28         49      0.957      0.996      0.994      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     30/149      1.97G     0.0305    0.01232   0.001523         20        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.89it/s]\n",
            "                   all         28         49      0.957          1      0.994      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     31/149      1.97G    0.03145     0.0116    0.00134         19        416: 100% 19/19 [00:02<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.87it/s]\n",
            "                   all         28         49      0.972      0.936       0.99      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     32/149      1.97G    0.03152    0.01201   0.001217         15        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.72it/s]\n",
            "                   all         28         49      0.935      0.974       0.99      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     33/149      1.97G    0.03136    0.01181   0.001511         20        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.15it/s]\n",
            "                   all         28         49      0.892      0.959      0.966      0.417\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     34/149      1.97G    0.03156    0.01284   0.001059         20        416: 100% 19/19 [00:02<00:00,  6.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.90it/s]\n",
            "                   all         28         49       0.94          1      0.992      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     35/149      1.97G    0.03111    0.01251   0.001073         21        416: 100% 19/19 [00:02<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.15it/s]\n",
            "                   all         28         49       0.94      0.987      0.983      0.437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     36/149      1.97G    0.02984    0.01134   0.001255         16        416: 100% 19/19 [00:02<00:00,  6.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.11it/s]\n",
            "                   all         28         49      0.963          1      0.995      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     37/149      1.97G    0.02984    0.01307   0.001705         21        416: 100% 19/19 [00:02<00:00,  6.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.29it/s]\n",
            "                   all         28         49      0.983      0.962      0.994      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     38/149      1.97G    0.02919    0.01173   0.001297         25        416: 100% 19/19 [00:02<00:00,  6.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.99it/s]\n",
            "                   all         28         49      0.974      0.985      0.995      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     39/149      1.97G    0.02912    0.01122  0.0009428         18        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.84it/s]\n",
            "                   all         28         49      0.968      0.987      0.995      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     40/149      1.97G    0.02959    0.01143  0.0009824         14        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.86it/s]\n",
            "                   all         28         49      0.981      0.987      0.995      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     41/149      1.97G    0.03095    0.01119   0.001273         14        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.00it/s]\n",
            "                   all         28         49      0.965      0.987      0.995      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     42/149      1.97G    0.02815     0.0117   0.001046         20        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.80it/s]\n",
            "                   all         28         49      0.954      0.987      0.992      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     43/149      1.97G    0.02881    0.01097   0.001049         12        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.99it/s]\n",
            "                   all         28         49      0.985      0.976      0.995      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     44/149      1.97G     0.0283    0.01143   0.001468         22        416: 100% 19/19 [00:02<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.85it/s]\n",
            "                   all         28         49      0.987      0.962      0.994      0.653\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     45/149      1.97G    0.02724    0.01175   0.001169         20        416: 100% 19/19 [00:02<00:00,  6.96it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.45it/s]\n",
            "                   all         28         49      0.983      0.983      0.995      0.622\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     46/149      1.97G    0.02687    0.01087   0.001236         19        416: 100% 19/19 [00:02<00:00,  6.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.82it/s]\n",
            "                   all         28         49      0.962          1      0.994       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     47/149      1.97G    0.02814    0.01218   0.001017         24        416: 100% 19/19 [00:02<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.24it/s]\n",
            "                   all         28         49      0.959      0.987      0.994       0.62\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     48/149      1.97G    0.02803    0.01215   0.001086         14        416: 100% 19/19 [00:02<00:00,  6.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.25it/s]\n",
            "                   all         28         49      0.955          1      0.994      0.615\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     49/149      1.97G    0.02656    0.01128  0.0008619         17        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.72it/s]\n",
            "                   all         28         49      0.955          1      0.994      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     50/149      1.97G    0.02727     0.0107  0.0008374         21        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.30it/s]\n",
            "                   all         28         49      0.954      0.996      0.994      0.632\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     51/149      1.97G    0.02635    0.01158  0.0007873         25        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.88it/s]\n",
            "                   all         28         49      0.953      0.995      0.994      0.555\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     52/149      1.97G    0.02691    0.01127  0.0008633         23        416: 100% 19/19 [00:02<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.01it/s]\n",
            "                   all         28         49      0.945      0.993      0.993      0.523\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     53/149      1.97G    0.02629    0.01057  0.0008747         23        416: 100% 19/19 [00:02<00:00,  6.92it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.02it/s]\n",
            "                   all         28         49       0.96      0.984      0.993      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     54/149      1.97G    0.02642    0.01095  0.0009354         16        416: 100% 19/19 [00:02<00:00,  6.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.71it/s]\n",
            "                   all         28         49      0.945          1      0.993      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     55/149      1.97G    0.02661    0.01086  0.0007943         18        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.90it/s]\n",
            "                   all         28         49      0.966      0.987      0.993      0.665\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     56/149      1.97G    0.02555    0.01035  0.0007835         14        416: 100% 19/19 [00:02<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.63it/s]\n",
            "                   all         28         49      0.971      0.983      0.993      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     57/149      1.97G    0.02597    0.01158  0.0007685         32        416: 100% 19/19 [00:02<00:00,  6.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.22it/s]\n",
            "                   all         28         49      0.961      0.987      0.993      0.656\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     58/149      1.97G     0.0244    0.01017  0.0008405         19        416: 100% 19/19 [00:02<00:00,  6.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.31it/s]\n",
            "                   all         28         49      0.965      0.975      0.994      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     59/149      1.97G    0.02564    0.01097  0.0007635         17        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.75it/s]\n",
            "                   all         28         49      0.957      0.984      0.993      0.671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     60/149      1.97G    0.02542     0.0106  0.0008488         13        416: 100% 19/19 [00:02<00:00,  6.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.69it/s]\n",
            "                   all         28         49      0.974      0.972       0.99      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     61/149      1.97G    0.02456    0.01055  0.0009515         19        416: 100% 19/19 [00:02<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.80it/s]\n",
            "                   all         28         49      0.975      0.974      0.994      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     62/149      1.97G    0.02429     0.0108  0.0009206         22        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.30it/s]\n",
            "                   all         28         49      0.981       0.97      0.994       0.57\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     63/149      1.97G    0.02423    0.01004  0.0007489         18        416: 100% 19/19 [00:02<00:00,  6.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.15it/s]\n",
            "                   all         28         49      0.957          1      0.994      0.656\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     64/149      1.97G    0.02442    0.01005  0.0006918         19        416: 100% 19/19 [00:02<00:00,  6.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.79it/s]\n",
            "                   all         28         49      0.962      0.997      0.994      0.672\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     65/149      1.97G    0.02534    0.01145  0.0006441         13        416: 100% 19/19 [00:02<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.92it/s]\n",
            "                   all         28         49      0.959      0.977      0.994      0.719\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     66/149      1.97G    0.02325    0.01026  0.0007144         10        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.08it/s]\n",
            "                   all         28         49      0.954      0.985      0.994      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     67/149      1.97G     0.0235    0.01008  0.0006288         15        416: 100% 19/19 [00:02<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.26it/s]\n",
            "                   all         28         49      0.947      0.993      0.994       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     68/149      1.97G     0.0236    0.01009  0.0005978         17        416: 100% 19/19 [00:02<00:00,  6.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.74it/s]\n",
            "                   all         28         49      0.977      0.951      0.991      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     69/149      1.97G    0.02398    0.01047  0.0006172         26        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.74it/s]\n",
            "                   all         28         49      0.946      0.993      0.994      0.586\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     70/149      1.97G    0.02335    0.01022  0.0009255         16        416: 100% 19/19 [00:02<00:00,  6.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.41it/s]\n",
            "                   all         28         49      0.946      0.983      0.993        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     71/149      1.97G    0.02359    0.01035  0.0006181         13        416: 100% 19/19 [00:02<00:00,  6.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.82it/s]\n",
            "                   all         28         49      0.949      0.993      0.994      0.666\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     72/149      1.97G    0.02265   0.009997  0.0005962         23        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.67it/s]\n",
            "                   all         28         49      0.947      0.998      0.994      0.671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     73/149      1.97G    0.02352     0.0103  0.0006136         27        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.74it/s]\n",
            "                   all         28         49      0.949      0.997      0.994      0.627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     74/149      1.97G    0.02266    0.01056  0.0006962         19        416: 100% 19/19 [00:02<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.63it/s]\n",
            "                   all         28         49      0.956      0.972      0.993      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     75/149      1.97G    0.02238   0.009861  0.0005115         20        416: 100% 19/19 [00:02<00:00,  6.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.89it/s]\n",
            "                   all         28         49      0.938      0.997      0.993      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     76/149      1.97G    0.02213    0.01003  0.0005834         22        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.91it/s]\n",
            "                   all         28         49      0.937          1      0.993      0.669\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     77/149      1.97G    0.02209   0.009687  0.0006221         12        416: 100% 19/19 [00:02<00:00,  6.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.75it/s]\n",
            "                   all         28         49      0.939          1      0.993      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     78/149      1.97G    0.02157   0.009451  0.0005391         24        416: 100% 19/19 [00:02<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.85it/s]\n",
            "                   all         28         49      0.955      0.997      0.994      0.673\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     79/149      1.97G    0.02088   0.009599   0.000604         11        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.73it/s]\n",
            "                   all         28         49       0.95      0.995      0.994      0.641\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     80/149      1.97G    0.02231   0.009011  0.0005574         16        416: 100% 19/19 [00:02<00:00,  6.97it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.44it/s]\n",
            "                   all         28         49       0.96       0.99      0.994      0.568\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     81/149      1.97G    0.02099   0.009876  0.0004713         18        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.79it/s]\n",
            "                   all         28         49      0.953      0.997      0.994      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     82/149      1.97G    0.02114   0.009336  0.0005172         13        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.98it/s]\n",
            "                   all         28         49      0.957      0.998      0.994      0.689\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     83/149      1.97G    0.02163   0.009649   0.000561         18        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.86it/s]\n",
            "                   all         28         49      0.953          1      0.994      0.672\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     84/149      1.97G    0.02244    0.01047  0.0007122         26        416: 100% 19/19 [00:02<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.00it/s]\n",
            "                   all         28         49      0.954      0.998      0.994      0.711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     85/149      1.97G    0.02195   0.009926  0.0005097         18        416: 100% 19/19 [00:02<00:00,  6.87it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.27it/s]\n",
            "                   all         28         49      0.955      0.997      0.994      0.687\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     86/149      1.97G    0.02089   0.009731  0.0004788         21        416: 100% 19/19 [00:02<00:00,  6.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.72it/s]\n",
            "                   all         28         49       0.94          1      0.993      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     87/149      1.97G    0.02067   0.009094  0.0005663         15        416: 100% 19/19 [00:02<00:00,  6.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.10it/s]\n",
            "                   all         28         49      0.984      0.959      0.994      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     88/149      1.97G    0.02096   0.009487  0.0004992         23        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.67it/s]\n",
            "                   all         28         49      0.953      0.987      0.993      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     89/149      1.97G    0.02043   0.009132  0.0006882         17        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.03it/s]\n",
            "                   all         28         49      0.952          1      0.994      0.688\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     90/149      1.97G    0.02059   0.009186  0.0005961         14        416: 100% 19/19 [00:02<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.69it/s]\n",
            "                   all         28         49      0.952      0.997      0.994       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     91/149      1.97G    0.02017   0.009417  0.0004681         21        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.02it/s]\n",
            "                   all         28         49      0.955      0.994      0.994      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     92/149      1.97G    0.02004   0.009225  0.0004219         20        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.61it/s]\n",
            "                   all         28         49      0.951      0.986      0.993      0.643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     93/149      1.97G    0.02041   0.009177  0.0006176         18        416: 100% 19/19 [00:02<00:00,  6.99it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.54it/s]\n",
            "                   all         28         49      0.937          1      0.993      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     94/149      1.97G    0.01947   0.009915  0.0003719         22        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.63it/s]\n",
            "                   all         28         49      0.941          1      0.994      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     95/149      1.97G    0.02028   0.009197  0.0005923         15        416: 100% 19/19 [00:02<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.81it/s]\n",
            "                   all         28         49      0.954      0.996      0.994      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     96/149      1.97G    0.01982   0.009427  0.0004207         21        416: 100% 19/19 [00:02<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.58it/s]\n",
            "                   all         28         49      0.953          1      0.994      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     97/149      1.97G    0.01961   0.009579  0.0004071         14        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.56it/s]\n",
            "                   all         28         49      0.956          1      0.994      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     98/149      1.97G     0.0192   0.009342  0.0003857         22        416: 100% 19/19 [00:02<00:00,  6.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.82it/s]\n",
            "                   all         28         49      0.956      0.996      0.994      0.641\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "     99/149      1.97G    0.01921   0.009159  0.0004549         11        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.83it/s]\n",
            "                   all         28         49      0.956      0.997      0.994      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    100/149      1.97G    0.01865   0.009225  0.0003991         21        416: 100% 19/19 [00:02<00:00,  6.84it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.46it/s]\n",
            "                   all         28         49      0.986      0.962      0.994      0.676\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    101/149      1.97G    0.01873   0.009821  0.0003963         24        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.73it/s]\n",
            "                   all         28         49      0.988      0.959      0.994       0.68\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    102/149      1.97G    0.01944   0.008849   0.000517         25        416: 100% 19/19 [00:02<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.57it/s]\n",
            "                   all         28         49      0.953      0.997      0.994      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    103/149      1.97G    0.01876   0.008626  0.0003405         22        416: 100% 19/19 [00:02<00:00,  6.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.91it/s]\n",
            "                   all         28         49      0.951          1      0.994      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    104/149      1.97G    0.01859   0.008779  0.0003907         15        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.87it/s]\n",
            "                   all         28         49       0.95          1      0.994      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    105/149      1.97G    0.01895   0.009383  0.0005598         30        416: 100% 19/19 [00:02<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.54it/s]\n",
            "                   all         28         49      0.952          1      0.994      0.687\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    106/149      1.97G    0.01831   0.009171  0.0002942         19        416: 100% 19/19 [00:02<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.69it/s]\n",
            "                   all         28         49      0.956      0.997      0.994      0.653\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    107/149      1.97G    0.01909    0.00891  0.0004466         20        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.05it/s]\n",
            "                   all         28         49      0.948          1      0.994      0.664\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    108/149      1.97G     0.0178   0.009218  0.0003632         18        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.54it/s]\n",
            "                   all         28         49      0.926      0.995       0.99      0.697\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    109/149      1.97G    0.01806   0.009114  0.0004902         20        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.50it/s]\n",
            "                   all         28         49       0.95          1      0.994      0.648\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    110/149      1.97G    0.01796   0.009096  0.0004541         11        416: 100% 19/19 [00:02<00:00,  6.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.99it/s]\n",
            "                   all         28         49      0.951          1      0.994      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    111/149      1.97G     0.0173   0.008116  0.0003755         17        416: 100% 19/19 [00:02<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.13it/s]\n",
            "                   all         28         49      0.949      0.997      0.994      0.661\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    112/149      1.97G    0.01718   0.008341   0.000279         11        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.57it/s]\n",
            "                   all         28         49      0.947      0.996      0.994       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    113/149      1.97G     0.0175   0.008746  0.0004036         19        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.15it/s]\n",
            "                   all         28         49      0.955      0.987      0.994      0.684\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    114/149      1.97G    0.01782   0.008373   0.000259         19        416: 100% 19/19 [00:02<00:00,  6.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.48it/s]\n",
            "                   all         28         49      0.952      0.997      0.994      0.674\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    115/149      1.97G     0.0171   0.008606  0.0002355         13        416: 100% 19/19 [00:02<00:00,  6.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.90it/s]\n",
            "                   all         28         49       0.95      0.997      0.994      0.695\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    116/149      1.97G    0.01681   0.008423  0.0002799         22        416: 100% 19/19 [00:02<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.20it/s]\n",
            "                   all         28         49      0.952      0.984      0.993      0.682\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    117/149      1.97G    0.01859   0.008851  0.0003904         21        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.87it/s]\n",
            "                   all         28         49       0.95      0.986      0.994      0.718\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    118/149      1.97G    0.01666    0.00841  0.0004485         15        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.52it/s]\n",
            "                   all         28         49      0.936      0.998      0.992      0.686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    119/149      1.97G    0.01697   0.008112  0.0002359         11        416: 100% 19/19 [00:02<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.51it/s]\n",
            "                   all         28         49      0.949      0.993      0.994      0.669\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    120/149      1.97G    0.01702   0.008339   0.000269         15        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.18it/s]\n",
            "                   all         28         49       0.95      0.993      0.994      0.693\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    121/149      1.97G    0.01689   0.008279  0.0003065         15        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.55it/s]\n",
            "                   all         28         49      0.948          1      0.994      0.712\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    122/149      1.97G    0.01603   0.008484  0.0003087         14        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.78it/s]\n",
            "                   all         28         49      0.955      0.998      0.994      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    123/149      1.97G    0.01666   0.008788  0.0002911         17        416: 100% 19/19 [00:02<00:00,  6.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.96it/s]\n",
            "                   all         28         49      0.955      0.998      0.994      0.674\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    124/149      1.97G    0.01654   0.008266   0.000458         14        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.25it/s]\n",
            "                   all         28         49      0.956      0.999      0.994      0.669\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    125/149      1.97G    0.01642   0.008027  0.0002578         23        416: 100% 19/19 [00:02<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.53it/s]\n",
            "                   all         28         49      0.953          1      0.994       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    126/149      1.97G    0.01662   0.008701  0.0002806         20        416: 100% 19/19 [00:02<00:00,  6.65it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.21it/s]\n",
            "                   all         28         49      0.977      0.959      0.994      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    127/149      1.97G    0.01642   0.008167  0.0002738         18        416: 100% 19/19 [00:02<00:00,  6.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.05it/s]\n",
            "                   all         28         49      0.975      0.959      0.993      0.677\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    128/149      1.97G     0.0162   0.008608  0.0002623         17        416: 100% 19/19 [00:02<00:00,  6.76it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.92it/s]\n",
            "                   all         28         49      0.947      0.996      0.994      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    129/149      1.97G    0.01633   0.008628   0.000263          9        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.97it/s]\n",
            "                   all         28         49      0.943          1      0.994      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    130/149      1.97G    0.01655   0.008401  0.0002055         17        416: 100% 19/19 [00:02<00:00,  6.69it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.56it/s]\n",
            "                   all         28         49      0.982      0.946      0.993      0.698\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    131/149      1.97G    0.01577   0.008845    0.00018         22        416: 100% 19/19 [00:02<00:00,  6.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.74it/s]\n",
            "                   all         28         49       0.98      0.946      0.993      0.686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    132/149      1.97G    0.01639   0.008362  0.0002725         15        416: 100% 19/19 [00:02<00:00,  6.88it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.40it/s]\n",
            "                   all         28         49      0.934      0.999      0.993      0.696\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    133/149      1.97G    0.01632   0.008183  0.0002857         17        416: 100% 19/19 [00:02<00:00,  6.62it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.65it/s]\n",
            "                   all         28         49      0.946      0.994      0.994      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    134/149      1.97G    0.01564   0.008543  0.0001761         26        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.50it/s]\n",
            "                   all         28         49      0.945      0.993      0.994       0.71\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    135/149      1.97G    0.01546   0.008363  0.0002689         24        416: 100% 19/19 [00:02<00:00,  6.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.91it/s]\n",
            "                   all         28         49      0.947      0.985      0.994      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    136/149      1.97G    0.01502   0.008487  0.0003061         22        416: 100% 19/19 [00:02<00:00,  6.85it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.48it/s]\n",
            "                   all         28         49      0.948      0.987      0.994      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    137/149      1.97G    0.01542   0.007942  0.0003625         17        416: 100% 19/19 [00:02<00:00,  6.78it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.82it/s]\n",
            "                   all         28         49      0.949      0.997      0.994      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    138/149      1.97G    0.01584    0.00857  0.0001594         17        416: 100% 19/19 [00:02<00:00,  6.80it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.69it/s]\n",
            "                   all         28         49      0.949      0.996      0.994      0.666\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    139/149      1.97G    0.01624   0.007875  0.0002025         12        416: 100% 19/19 [00:02<00:00,  6.79it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.67it/s]\n",
            "                   all         28         49       0.95      0.996      0.994      0.671\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    140/149      1.97G    0.01561   0.007962   0.000231         15        416: 100% 19/19 [00:02<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.88it/s]\n",
            "                   all         28         49       0.95      0.996      0.994      0.668\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    141/149      1.97G    0.01487   0.008076  0.0002131         17        416: 100% 19/19 [00:02<00:00,  6.95it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.03it/s]\n",
            "                   all         28         49      0.949      0.999      0.994      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    142/149      1.97G    0.01446   0.007971  0.0003166         15        416: 100% 19/19 [00:02<00:00,  6.60it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.90it/s]\n",
            "                   all         28         49      0.944          1      0.994      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    143/149      1.97G    0.01532    0.00854  0.0002109         20        416: 100% 19/19 [00:02<00:00,  6.67it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  6.02it/s]\n",
            "                   all         28         49      0.947          1      0.994      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    144/149      1.97G    0.01478   0.007832  0.0001487         17        416: 100% 19/19 [00:02<00:00,  6.93it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.56it/s]\n",
            "                   all         28         49      0.949      0.986      0.994      0.693\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    145/149      1.97G    0.01474   0.007991   0.000188         23        416: 100% 19/19 [00:02<00:00,  6.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.58it/s]\n",
            "                   all         28         49      0.947          1      0.994      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    146/149      1.97G    0.01512   0.008094  0.0002421         28        416: 100% 19/19 [00:02<00:00,  6.81it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.21it/s]\n",
            "                   all         28         49      0.945          1      0.994      0.707\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    147/149      1.97G     0.0153    0.00742  0.0001669         14        416: 100% 19/19 [00:02<00:00,  6.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.89it/s]\n",
            "                   all         28         49      0.946          1      0.994      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    148/149      1.97G    0.01556   0.008322  0.0001765         13        416: 100% 19/19 [00:02<00:00,  6.77it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.47it/s]\n",
            "                   all         28         49      0.948          1      0.994      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "    149/149      1.97G    0.01507   0.007774  0.0001366         19        416: 100% 19/19 [00:02<00:00,  6.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  5.91it/s]\n",
            "                   all         28         49      0.947          1      0.994      0.707\n",
            "\n",
            "150 epochs completed in 0.141 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.3MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.3MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  4.46it/s]\n",
            "                   all         28         49      0.959      0.977      0.994      0.721\n",
            "                 Amjad         28         10      0.943          1      0.995      0.758\n",
            "                 Other         28         39      0.974      0.955      0.993      0.684\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights runs/train/exp/weights/amjad.pt --img 416 --conf 0.1 --source 'C:\\Users\\User\\Downloads\\FaceTest\\facetest.mp4'"
      ],
      "metadata": {
        "id": "h4GurrCliV77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb241033-156f-44bb-e841-fe3a318c0f5a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/amjad.pt'], source=C:\\Users\\User\\Downloads\\FaceTest\\facetest.mp4, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v6.2-187-g5ef69ef Python-3.7.14 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 258, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 253, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 108, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
            "  File \"/content/yolov5/yolov5/utils/dataloaders.py\", line 251, in __init__\n",
            "    raise FileNotFoundError(f'{p} does not exist')\n",
            "FileNotFoundError: /content/yolov5/yolov5/C:\\Users\\User\\Downloads\\FaceTest\\facetest.mp4 does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThArDozfigBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch-yolo",
      "language": "python",
      "name": "pytorch-yolo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd15c865fd0b44389aaaab9dd01400b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe04b5a5e99547dcaf0efcfc42783bb5",
              "IPY_MODEL_84b12ece4ac2425f8a8df7559471d260",
              "IPY_MODEL_0fce803cfb3048308f0e60830a8b1a55"
            ],
            "layout": "IPY_MODEL_bf1a697c93854a76bc6b33ab17c109ed"
          }
        },
        "fe04b5a5e99547dcaf0efcfc42783bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aca63cd1c1de4edca996d116f3bbf63c",
            "placeholder": "​",
            "style": "IPY_MODEL_a7adca183b6a4d95a283a61070831910",
            "value": "100%"
          }
        },
        "84b12ece4ac2425f8a8df7559471d260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad8a540818f1463198a35f1f3a3a614d",
            "max": 114419221,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8b13bdcbdd149c7a2fe43fd5e884dc9",
            "value": 114419221
          }
        },
        "0fce803cfb3048308f0e60830a8b1a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6234a07ab18488e96d61ee410724c5e",
            "placeholder": "​",
            "style": "IPY_MODEL_b8e9a21398d34509919ef007397a7d8e",
            "value": " 109M/109M [00:01&lt;00:00, 96.1MB/s]"
          }
        },
        "bf1a697c93854a76bc6b33ab17c109ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca63cd1c1de4edca996d116f3bbf63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7adca183b6a4d95a283a61070831910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad8a540818f1463198a35f1f3a3a614d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8b13bdcbdd149c7a2fe43fd5e884dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6234a07ab18488e96d61ee410724c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8e9a21398d34509919ef007397a7d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95db33b2867b4d9aa1762c09816ff54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc59315ff3f043bb8d3adf96da7aafc1",
              "IPY_MODEL_51e6ad45015a46b5ba1cdf0c56df2f5d",
              "IPY_MODEL_f8c731b0d0e44de8b6b5806f357e0f55"
            ],
            "layout": "IPY_MODEL_441dcea1e5dc44228058b148a1633460"
          }
        },
        "fc59315ff3f043bb8d3adf96da7aafc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4d97d9a3afd42509a725edf2e0166f6",
            "placeholder": "​",
            "style": "IPY_MODEL_fba2092975c64efca2f8091b39523fe8",
            "value": "100%"
          }
        },
        "51e6ad45015a46b5ba1cdf0c56df2f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2686506dafc14a7c8d09b8c22efc1f5f",
            "max": 52147035,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d650a14214a14c49a03c08b1fbe8d308",
            "value": 52147035
          }
        },
        "f8c731b0d0e44de8b6b5806f357e0f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eb58b1a43bd462bac605146ff7e0e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_fdb581bfe5cd46848a01489bc8af0908",
            "value": " 49.7M/49.7M [00:00&lt;00:00, 94.9MB/s]"
          }
        },
        "441dcea1e5dc44228058b148a1633460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4d97d9a3afd42509a725edf2e0166f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba2092975c64efca2f8091b39523fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2686506dafc14a7c8d09b8c22efc1f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d650a14214a14c49a03c08b1fbe8d308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7eb58b1a43bd462bac605146ff7e0e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb581bfe5cd46848a01489bc8af0908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}